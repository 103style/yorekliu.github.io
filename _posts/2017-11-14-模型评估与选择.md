---
categories:
  - Machine Learning
toc: true
toc_label: "目录"
toc_icon: "heart"
---

#模型评估与选择

### 2.1 经验误差与过拟合

通常我们把分类错误的样本数占样本总数的比例称为“错误率(error rate)”，即如果在m个样本中有a个样本分类错误，则错误率E=a/m；相应的，1-a/m称为精度accuracy。更一般地，我们把学习期的实际预测输出与样本的真实输出之间的差异称为“误差error”，学习期在训练集上的误差称为“训练误差training error”或者“经验误差empirical error”，在新样本上的误差称为“泛化误差generalization error”。

学习器把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就导致了泛化性能下降，这种现象叫做过拟合overfittting；与之相对的欠拟合underfitting，这是指对训练样本的一般性质尚未学好。

### 2.2 评估方法

通常，我们可通过实验测试来对学习器的泛化错误进行评估并进而做出选择。因此，我们需要使用一个测试集testing set来测试学习器对新样本的判别能力，然后以测试集的测试误差testing error作为泛化误差的近似。

#### 2.2.1 留出法

留出法hold-out直接将数据集$\mathit{D}$划分为两个互斥的集合，其中一个作为训练集$\mathit{S}$，另外一个作为测试集$\mathit{T}$，即，$\mathit{D}=\mathit{S}\cup\mathit{T}$，$\mathit{S}\cap\mathit{T}=\emptyset$。

训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果长生影响。从采样sampling的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称之为分层采样stratified sampling。

需要注意的是，即使给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集$\mathit{D}$进行分割。单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果，留出法返回的则是这100个结果的平均。

因为留出法需要划分训练/测试集，所以如果训练集$\mathit{S}$包含绝大多数样本，测试集$\mathit{T}$比较小，会导致训练出的模型更接近于用$\mathit{D}$训练出来的模型，而且评估结果可能不够稳定准确；反之，被评估模型与用$\mathit{D}$训练出来的模型相比可能有较大差异，从而降低评估结果的保真值fidelity。这个问题没有完美的解决方法，常见做法是将大约2/3～4/5的样本用于训练，剩余用户测试。一般而言，测试集至少需要30个样例。

#### 2.2.2 交叉验证法
