---
categories:
  - Machine Learning
toc: true
toc_label: "目录"
toc_icon: "heart"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 1 经验误差与过拟合

通常我们把分类错误的样本数占样本总数的比例称为“错误率(error rate)”，即如果在m个样本中有a个样本分类错误，则错误率E=a/m；相应的，1-a/m称为精度accuracy。更一般地，我们把学习期的实际预测输出与样本的真实输出之间的差异称为“误差error”，学习期在训练集上的误差称为“训练误差training error”或者“经验误差empirical error”，在新样本上的误差称为“泛化误差generalization error”。

学习器把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就导致了泛化性能下降，这种现象叫做过拟合overfittting；与之相对的欠拟合underfitting，这是指对训练样本的一般性质尚未学好。

## 2 评估方法

通常，我们可通过实验测试来对学习器的泛化错误进行评估并进而做出选择。因此，我们需要使用一个测试集testing set来测试学习器对新样本的判别能力，然后以测试集的测试误差testing error作为泛化误差的近似。

### 2.1 留出法

留出法hold-out直接将数据集$$\mathit{D}$$划分为两个互斥的集合，其中一个作为训练集$$\mathit{S}$$，另外一个作为测试集$$\mathit{T}$$，即，$$\mathit{D}=\mathit{S}\cup\mathit{T}$$，$$\mathit{S}\cap\mathit{T}=\emptyset$$。

训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果长生影响。从采样sampling的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称之为分层采样stratified sampling。

需要注意的是，即使给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集$$\mathit{D}$$进行分割。单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果，留出法返回的则是这100个结果的平均。

因为留出法需要划分训练/测试集，所以如果训练集$$\mathit{S}$$包含绝大多数样本，测试集$$\mathit{T}$$比较小，会导致训练出的模型更接近于用$$\mathit{D}$$训练出来的模型，而且评估结果可能不够稳定准确；反之，被评估模型与用$$\mathit{D}$$训练出来的模型相比可能有较大差异，从而降低评估结果的保真值fidelity。这个问题没有完美的解决方法，常见做法是将大约2/3～4/5的样本用于训练，剩余用户测试。一般而言，测试集至少需要30个样例。

### 2.2 交叉验证法

交叉验证法cross validation先将数据集$$\mathit{D}$$划分为k个大小相似的互斥子集，即$$\mathit{D}=\mathit{D_1}\cup\mathit{D_2\cup...}\cup\mathit{D_k},\mathit{D_i}\cap\mathit{D_j}=\emptyset(\mathit{i}\ne\mathit{j})$$.每个子集都尽可能保持数据分布的一致性，即从$$\mathit{D}$$中通过分层采样得到。然后，每次用k-1个子集作为训练集，余下的子集作为测试集；这样就可以获得k组训练/测试集，从而可以进行k次训练和测试，最终结果就是这k个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性很大程度取决于k的均值，为了强调这一点，通常把交叉验证法称为"k折交叉验证(k-fold cross validation)"。k最常用的取值是10，此时称为10折交叉验证；其他常用的是5、20等.

与留出法相似，将数据集$$\mathit{D}$$划分为k个子集也存在多种划分方式，为了减小因样本划分不同而引入的差异，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证额均值。

假设数据集$$\mathit{D}$$中包含m个样本，令k=m，得到了交叉验证法的一个特例：留一法Leave-One-Out, LOO。留一法使用的训练集与初始数据集相比只少了一个样本，这就使得绝大多数情况下，留一法中被实际评估的模型与期望评估的用$$\mathit{D}$$训练出来的模型很相似，因此，留一法的评估结果往往被认为比较准确。然而，留一法在数据集比较大时，训练m个模型的计算开销可能是难以忍受的；另外，留一法的估计结果也未必永远比其他评估方法准确；NFL定理对实验评估方法同样适用。

### 2.3 自助法

上面两种方法保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比$$\mathit{D}$$小，这必然会引入误差。而留一法受训练样本规模变化较小，但计算复杂度又太高了。

“自助法bootstrapping”是一个比较好的解决方案，它直接以自助采样法bootstrap sampling为基础。给定包含m个样本的数据集$$\mathit{D}$$，我们对它进行采样产生数据集$$\mathit{D^{'}}$$：每次随机从$$\mathit{D}$$中挑选一个样本，将其拷贝放入$$\mathit{D^{'}}$$，然后放回出事数据集$$\mathit{D}$$中，重复操作m次，这样我们就得到了包含m个样本的数据集$$\mathit{D^{'}}$$，这就是自助采样的结果。

样本在m次采样中始终不被采到的概率是$$(1-1/m)^{m}$$,取极限得到

$$
{\lim_{m \to +\infty}}(1-1/m)^{m}\to\frac{1}{e}\approx0.368
$$

通过自助采样，初始数据集$$\mathit{D}$$中约有36.8%的样本未出现在采样数据集$$\mathit{D^{'}}$$中，于是我们可以将$$\mathit{D^{'}}$$用作训练集，$$\mathit{D}\setminus\mathit{D^{'}}$$用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有1/3的没有在训练集中出现的样本用于测试。这样的测试结果，也称包外估计out-of-bag estimate。

自助法在数据集较小、难以有效划分训练/测试集时很有用。此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始化数据量足够时，留出法和交叉验证法更常用一些。

## 3 性能度量

对学习器的泛化能力进行评估，需要有衡量模型泛化能力的评价标准，这就是性能度量performance measure。

在预测任务中，给定样例集$$\mathit{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}$$,其中y_i是示例x_i的真实标记，要评估学习器f的性能，就要把学习器预测结果f(x)与真实标记y进行比较。

回归任务最常用的性能度量是均方误差mean squared error

$$
\mathit{E(f; D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2}
$$

更一般的，对于数据分布$$\mathit{D}$$和概率密度函数$$\mathit{p}(·)$$，均方误差可以描述为

$$
E(f; D)=\int_{x～D}{}(f(x)-y)^{2}p(x)dx
$$

### 3.1 错误率与精度

对于样例集$$\mathit{D}$$，分类错误率定义为

$$
E(f; D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(f(x_i)\ne{y_i})
$$

精度定义为

$$
acc(f; D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(f(x_i)={y_i})=1-E(f; D)
$$

更一般的，对于数据分布$$\mathit{D}$$和概率密度函数$$\mathit{p}(·)$$，错误率与精度可以分别描述为

$$
E(f; D)=\int_{x～D}\mathbb{I}(f(x)\ne{y})p(x)dx
$$

$$
acc(f; D)=\int_{x～D}\mathbb{I}(f(x)=y)p(x)dx=1-E(f; D)
$$

### 3.2 查准率、查全率与F1

查准率precision表示检索出的相关文献量与检索出的文献总量的百分比，也称准确率。查全率recall指检索出的相关文献量与检索系统中相关文献总量的百分比，也称召回率。这两者是一个矛盾的度量。

对于二分类问题，可将样例根据其真实类别与学习器预测类型的组合化分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)。令TP、FP、TN、FN分别表示对应的样例数，这样的分类结果可以组成一个混淆矩阵confusion matrix

  真实情况\预测结果	正例   	反例   
  正例       	真正例TP	假反例FN
  反例       	假正例FP	真反例TN

查准率P与查全率R分别定义如下：

$$
P=\frac{TP}{TP+FP}
$$

$$
R=\frac{TP}{TP+FN}
$$

在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本。
